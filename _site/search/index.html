<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- JQuery (used for bootstrap and jekyll search) -->
    <script src="/mckaydev.github.io/assets/js/jquery-3.2.1.min.js" ></script>
    
    <!-- Main JS (navbar.js and katex_init.js)-->
    <script defer=true src="/mckaydev.github.io/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/mckaydev.github.io/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/mckaydev.github.io/assets/favicon.ico" type="image/x-icon">

    <!-- Canonical -->
    <link rel="canonical" href="http://localhost:4000/mckaydev.github.io/search/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="Mckay's Dev Blog" href="http://localhost:4000//mckaydev.github.io//feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/mckaydev.github.io/assets/css/font-awesome.min.css">

    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <!--<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script> -->
    <link rel="stylesheet" type="text/css" href="/mckaydev.github.io/assets/css/katex.min.css">
    <script src="/mckaydev.github.io/assets/js/katex.min.js">
    </script>
    

    <!-- Google Analytics -->
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | Mckay’s Dev Blog</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A website with Mckay’s activity" />
<meta property="og:description" content="A website with Mckay’s activity" />
<link rel="canonical" href="http://localhost:4000/mckaydev.github.io/search/" />
<meta property="og:url" content="http://localhost:4000/mckaydev.github.io/search/" />
<meta property="og:site_name" content="Mckay’s Dev Blog" />
<script type="application/ld+json">
{"description":"A website with Mckay’s activity","@type":"WebPage","url":"http://localhost:4000/mckaydev.github.io/search/","headline":"Search","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Manual seo tags -->
    <!--
    <title>Search | Mckay's Dev Blog</title>
    <meta name="description" content="">
    -->
</head>

  <body>
    <header class="site-header">

    <!-- Logo and title -->
	<div class="branding">
		<a href="/mckaydev.github.io/">
			<img class="avatar" src="/mckaydev.github.io/assets/img/triangle.svg" alt=""/>
		</a>

		<h1 class="site-title">
			<a href="/mckaydev.github.io/">Mckay's Dev Blog</a>
		</h1>
	</div>

    <!-- Toggle menu -->
    <nav class="clear">
    <a id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>

    <!-- Menu -->
    <ul>
        
        
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="/mckaydev.github.io/about/">
                About
            </a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="http://localhost:4000/mckaydev.github.io/portfolio">
                TensorFlow
            </a>
        </li>
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="http://localhost:4000/mckaydev.github.io/search">
                <i class="fa fa-search" aria-hidden="true"></i>
            </a>
        </li>
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="http://localhost:4000/mckaydev.github.io/tags">
                <i class="fa fa-tags" aria-hidden="true"></i>
            </a>
        </li>
        

    </ul>

	</nav>
</header>

    <div class="content">
      <article class="feature-image">
  <header id="main" style="background-image: url('/mckaydev.github.io/assets/img/pexels/search-map.jpeg')">
    <h1 id="Search" class="title">
        Search
    </h1>
    
    
    <h2 class="subtitle">What are you looking for?</h2>
    
      
  </header>
  <section class="post-content"><!-- Html Elements for Search -->
<input type="text" id="search-input" placeholder="Enter keywords..." class="search-bar">
<br>
<br>
<ul id="results-container"></ul>

<section>
    <!-- Script pointing to jekyll-search.js -->
    <script src="/mckaydev.github.io/assets/js/simple-jekyll-search.min.js" type="text/javascript"></script>

    <script type="text/javascript">
        SimpleJekyllSearch({
            searchInput: document.getElementById('search-input'),
            resultsContainer: document.getElementById('results-container'),
            json: [
                    
                     
                        {
                          "title"    : "Start Blogging",
                          "category" : "",
                          "tags"     : " ",
                          "url"      : "/mckaydev.github.io/2014/11/30/start-blogging.html",
                          "date"     : "November 30, 2014",
                          "excerpt"  : "공부가 주 목적인 블로그를 개설했습니다. 웹언어 전공이 아니고 배워본적도 없기때문에 서툴지만 차근차근 배워나가며 블로그를 이쁘게 만드려고 합니다. 아직 jekyll 테마의 기본에서 벗어나지 못했지만 계속해서 공부하고 발전시켜나가는 블로거가 되겠습니다.!wish? list  일주일에 한개 이상의 포스트  블로깅이 본업에 방해되지는 않아야 함.  건강챙기기  추구할 수 있는 용기가 있다면 우리의 모든 꿈은 이뤄질 수 있다. – 월트 디즈니",
                          "content"  : "공부가 주 목적인 블로그를 개설했습니다. 웹언어 전공이 아니고 배워본적도 없기때문에 서툴지만 차근차근 배워나가며 블로그를 이쁘게 만드려고 합니다. 아직 jekyll 테마의 기본에서 벗어나지 못했지만 계속해서 공부하고 발전시켜나가는 블로거가 되겠습니다.!wish? list  일주일에 한개 이상의 포스트  블로깅이 본업에 방해되지는 않아야 함.  건강챙기기  추구할 수 있는 용기가 있다면 우리의 모든 꿈은 이뤄질 수 있다. – 월트 디즈니"
                        } 
                     ,
                     
                       {
                         
                            "title"    : "Multi variable input &amp; Load data",
                            "category" : "",
                            "tags"     : " TensorFlow",
                            "url"      : "/mckaydev.github.io/portfolio/2mul-val",
                            "date"     : "February 15, 2013",
                            "excerpt"  : "",
                            "content"  : "&lt;Multi Variable&gt;import tensorflow as tfx_data = [[73., 80., 75.], [93., 88., 93.],          [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]y_data = [[152.], [185.], [180.], [196.], [142.]]# x1 = tf.placeholder(tf.float32)# x2 = tf.placeholder(tf.float32)# x3 = tf.placeholder(tf.float32)# Y = tf.placeholder(tf.float32)# placeholders for a tensor that will be always fed.# 이번 예제에선 5개의 x 데이터 셋을 입력했지만 여기선 미지수 None 으로 설정# x 데이터 셋 안의 데이터 갯수 73 80 75 ... 3개, y 는 한개X = tf.placeholder(tf.float32, shape=[None, 3])Y = tf.placeholder(tf.float32, shape=[None, 1])# w1 = tf.Variable(tf.random_normal([1]), name='weight1')# w2 = tf.Variable(tf.random_normal([1]), name='weight2')# w3 = tf.Variable(tf.random_normal([1]), name='weight3')# b = tf.Variable(tf.random_normal([1]), name='bias')# 원래는 W를 x의 갯수만큼 5개를 만들어 줘야 하지만 여기서는 [3, 1]을 통해 한문장으로W = tf.Variable(tf.random_normal([3, 1]), name='weight')# b는 전과 같이 여러개의 데이터를 입력하여도 하나이기 때문에 기존과 같음b = tf.Variable(tf.random_normal([1]), name='bias')# Hypothesis# 가설 또한  'x1 * w1 + x2 * w2 + x3 * w3 + b' 이 아닌 행렬 곱셈연산을 통하여 계산hypothesis = tf.matmul(X, W) + b# Simplified cost/loss functioncost = tf.reduce_mean(tf.square(hypothesis - Y))# Minimizeoptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)train = optimizer.minimize(cost)# Launch the graph in a session.sess = tf.Session()# Initializes global variables in the graph.sess.run(tf.global_variables_initializer())# for step in range(2001):#    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],#                         feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})#    if step % 10 == 0:#        print(step, \"Cost: \", cost_val, \"\nPrediction:\n\", hy_val)for step in range(2001):    cost_val, hy_val, _ = sess.run(        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})    if step % 10 == 0:        print(step, \"Cost: \", cost_val, \"\nPrediction:\n\", hy_val)&lt;Multi Variable input data&gt;Load-data-Prac-01.csv:# EXAM1,EXAM2,EXAM3,FINAL73,80,75,15293,88,93,18589,91,90,18096,98,100,19673,66,70,14253,46,55,101import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt# import os# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'tf.set_random_seed(777)  # for reproducibilityxy = np.loadtxt('Load-data-Prac-01.csv', delimiter=',', dtype=np.float32)# [:, 0:-1] 행은 다 가져오면서 x의 데이터는 첫번째부터 마지막의 이전이니깐 -1까지x_data = xy[:, 0:-1]# [:, 0:-1] 행은 다 가져오면서 y의 데이터는 마지막 한 열만 이니깐 -1만 가져오기y_data = xy[:, [-1]]# Make sure the shape and data are OKprint(\"--------------------------------\n\")# x_data.shape : 배열의 모양, x_data : 배열의 내용, len(x_data) : 배열의 길이(행의 갯수)print(x_data.shape, x_data, len(x_data))print(y_data.shape, y_data)print(\"--------------------------------\n\")# placeholder# (#     placeholders for a tensor that will be always fed.#     placeholder 자료형은 선언과 동시에 초기화 x, 일단 선언 후 그 다음 값을 전달한다.#     따라서 반드시 실행 시 데이터가 제공되어야 한다.#     여기서 값을 전달하는 것이 데이터를 상수값을 전달함과 같이 할당하는 것이 아니라#     다른 텐서(Tensor)를 placeholder 에 맵핑 시키는 것이라고 보면 된다.#     할당하기 위해 feed dictionary 를 활용, 세션을 생성시 feed_dict 의 키워드 형태로 텐서를 맵핑#     선언 후 feed_dict 변수를 할당해도 되고 바로 값을 대입시켜도 무방.#     dtype, : 데이터 타입을 의미하며 반드시 적어주어야 한다.#     shape=None, : 입력 데이터의 형태를 의미한다.#     상수 값이 될 수도 있고 다차원 배열의 정보가 들어올 수도 있다.#     ( 디폴트 파라미터로 None 지정 )#     name=None : 해당 placeholder 의 이름을 부여하는 것으로 적지 않아도 된다.#     ( 디폴트 파라미터로 None 지정 )# )X = tf.placeholder(tf.float32, shape=[None, 3])Y = tf.placeholder(tf.float32, shape=[None, 1])# Variable# (#     텐서가 아니라 하나의 객체#     Variable 클래스의 인스턴스가 생성되는것, 그리고 해당 인스턴스를 그래프에 추가시켜주어야 함##     실제 global_variables_initializer() 를 사용(호출), 이 자체가 연산이 된다.#     호출하기 전에 그래프의 상태는 각 노드에 값이 아직 없는 상태를 의미한다.#     따라서 해당 함수를 사용해주어야 Variable 의 값이 할당 되는 것이고 텐서의 그래프로써의 효력이 발생# )# tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)# shape: 정수값의 1-D 텐서 또는 파이썬 배열. 반환값 텐서의 shape 입니다.# mean: 0-D 텐서 또는 dtype 타입의 파이썬 값. 정규분포의 평균값.# stddev: 0-D 텐서 또는 dtype 타입의 파이썬 값. 정규분포의 표준 편차.# dtype: 반환값의 타입.# seed: 파이썬 정수. 분포의 난수 시드값을 생성하는데에 사용됩니다. 동작 방식은 set_random_seed 를 보십시오.# name: 연산의 명칭 (선택사항).W = tf.Variable(tf.random_normal([3, 1]), name='weight')b = tf.Variable(tf.random_normal([1]), name='bias')# Hypothesis# 텐서플로우에서 행렬의 곱셈은 * 를 사용하지 않고, 텐서플로우 함수“tf.matmul”을 사용한다.hypothesis = tf.matmul(X, W) + b# Simplified cost/loss function# 제곱 - square# 세제곱 - cube# N승 - to the power of N# 제곱근 - square root# 세제곱근 - cube root# 1. hypothesis 방정식에서 y 좌표의 값을 빼면, 단순 거리가 나온다.#    hypothesis - y_data 가 여기에 해당하고 hypothesis 와 y_data 모두 매트릭스. 즉, 행렬(벡터) 연산.# 2. 단순 거리는 음수 또는 양수이기 때문에 제곱을 해서 멀리 있는 데이터에 벌점을 부여한다.#    tf.square() - 매트릭스에 포함된 요소에 대해 각각 제곱하는 행렬 연산# 3. 합계에 대해 평균을 계산한다.#    tf.reduce_mean() - 합계 코드가 보이지 않아도 평균을 위해 내부적으로 합계 계산. 결과값은 실수 1개.cost = tf.reduce_mean(tf.square(hypothesis - Y))# Minimize# learning rate 중요하다. 이 값을 자동으로 알아낼 수는 없고, 여러 번에 걸쳐 테스트하면서 적절한 값을 찾아야 함.# 여기서는 0.00001을 사용. 이때 0.00001이 적용되는 대상은 기울기에 해당하는 W임. 나중에 그래프가 나올 때 확인할 수 있다.## gradient descent 알고리듬을 구현한 코드가 tf.train.GradientDescentOptimizer 함수.# \"경사타고 내려가기\"라는 미분을 통해 최저 비용을 향해 진행하도록 만드는 핵심 함수.# 이때 rate 를 전달했기에 W축에 대해서 매번 0.00001 만큼씩 내려가게 됨.## minimize 함수는 글자 그대로 최소 비용을 찾아주는 함수라고 생각. 그러나, 정확하게는 gradient descent 알고리즘에서 gradients 를 계산,# 변수에 적용하는 일을 동시에 하는 함수. W와 b를 적절하게 계산해서 변경하는데, 그 진행 방향이 cost 가 작아지는 쪽임.## 중요한 것은 train 텐서에 연결된 것이 정말 많다는 것. optimizer 는 직접 연결되었고, optimizer 에는 cost 와 rate 가 연결되었으니까# 이들은 한 다리 걸쳐 연결되었고, cost 에는 reduce_mean 과 square 함수를 통해 (hypothesis - y_data)의 결과가 두 다리 걸쳐# 연결되었고, hypothesis 는 W, x_data, b와 연결되었으므로 세 다리 걸쳐 연결된 상태라는 것이다. 그래서, train 을 구한다는 것은# 이 모든 연결된 객체들을 계산한다는 것과 같은 뜻이 된다.optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)# train = optimizer.minimize(cost)# Launch the graph in a session.# 텐서플로우는 연산하는 과정을 그래프로 만들어서 생성하게됨. 그것이 하나의 세션을 의미# ex) a -&gt; x -&gt; + -&gt; d#          l    l#          b    c# 1. Python 코드에서 텐서를 생성, 텐서를 통하여 연산을 정의.#    이는 곧 그래프의 생성과 동시에 세션이 할당됨을 의미한다.# 2. 생성된 세션이 연산장치(CPU, GPU) 에 의하여 연산이 할당 (Embedding) 시킨다.# 3. 펌웨어 레벨에서 고속 연산을 수행한다. 실제로 C에서 연산을 처리하는 것이며#    이를 통하여 파이썬의 속도한계를 극복한다.# 4. 그리고 연산의 결과가 반환된다.#       (Tensorflow_Architecture 참조)#       1. Client 단 에서는 데이터의 흐름을 그래프로 정의하여 세션을 만드는 것#       2. Distributed Master 는 Session.run() 을 통하여 그래프의 부분부분을#          나누어 분산 처리하여 각 Worker Services 에 보낸다.#       3. Worker Services 는 Distributed Master 단에서 받은 각 그래프의 부분조각의 작업을#          커널(Kernel) 단에서 처리하기 위해 스케쥴링을 실시한다.#       4. Kernel Implementations 에서는 Worker Services 에서 스케쥴된 작업 즉, 연산을 수행한다.# 그래프를 세션에 올리기 위해 Session 객체를 만듬.sess = tf.Session()# Initializes global variables in the graph.# 세션에 인자로 넘겨주고 세션의 'run()' 메소드를 호출.# 연산이 필요로 하는 모든 입력은 세션에 의해 자동으로 실행(일반적으로는 병렬으로 실행됨)# 자원을 시스템에 돌려주기 위해 세션을 닫아야 함. ('sess.close()')# with tf.Session() as sess:#   with tf.device(\"/gpu:1\"):# with tf.device('/cpu:0'):#     sess = tf.Session()#     sess.run(tf.global_variables_initializer())# 이처럼 cpu 나 gpu 여러개 중에서 무엇을 사용할 것인지 정해줄 수 있음# 변수는 그래프가 올라간 뒤 'init' 연산을 실행해서 반드시 초기화 되어야 합니다.# 그 전에 먼저 'init' 연산을 그래프에 추가해야 합니다.# ex)# init_op = tf.global_variables_initializer()# sess.run(init_op)sess.run(tf.global_variables_initializer())# Set up feed_dict variables inside the loop.for step in range(5001):    # 연산의 출력을 가져오기 위해 Session 객체에서 텐서를 run() 에 인자로 넘겨 그래프를 실행해야 함.    # 여러 개의 텐서를 가져올 수도 있음. (ex) result = sess.run([mul, intermed])    # 요청된 텐서들의 출력값을 만드는 데 관여하는 모든 연산은(요청된 텐서 하나 당 한 번이 x) 한 번만 실행    # 피드(feed) 는 연산의 출력을 지정한 텐서 값으로 임시 대체합니다.    # 임시로 사용할 피드 데이터는 run()의 인자로 넘겨줄 수 습니다.    # 피드데이터는 run 을 호출할 때 명시적 인자로 넘겨질 때만 사용됩니다.    # 흔한 사용법은 tf.placeholder() 를 사용해 특정 연산을 \"피드\" 연산으로 지정하는 것입니다.    # + placeholder() 연산은 피드 데이터를 제공하지 않으면 오류를 일으킵니다.    cost_val, hy_val, _ = sess.run([cost, hypothesis, optimizer], feed_dict={X: x_data, Y: y_data})    if step % 10 == 0:        # step : 몇번째 러닝인지, cost_val : 현재 코스트 값, hy_val : 현재까지의 데이터로 가설 계산후 결과        print(step, \"Cost: \", cost_val, \"\nPrediction:\n\", hy_val)# Ask my score# 원하는 데이터 값을 준 뒤, 그 데이터를 가설에 넣어서 계산하라고 세션에 명령해줌.print(\"Your score will be \", sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))print(\"Other scores will be \", sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))# plt.plot((W[0] + W[1] + W[2]) / 3, cost_val, 'ro')# plt.xlabel('W')# plt.ylabel('Cost')# plt.show()"
                         
                       } ,
                     
                       {
                         
                            "title"    : "Logistic Regression",
                            "category" : "",
                            "tags"     : " TensorFlow",
                            "url"      : "/mckaydev.github.io/portfolio/1logi-reg",
                            "date"     : "March 1, 2013",
                            "excerpt"  : "",
                            "content"  : "&lt;Logistic Regression&gt;import tensorflow as tf# import os# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'# 두 가지 분류를 활용할 수 있는 몇 가지 예제를 설명하고 있다.# 스팸 메일 탐지, 페이스북 피드 표시, 신용카드 부정 사용은 두 가지 값 중의 하나를  선택하게 된다.# 프로그래밍에서는 이 값을 boolean 이라고 부르지만, 여기서는 쉽게 1과 0으로 구분한다.  1은 spam, show, fraud 에 해당.# 1과 0에 특별한 값을 할당하도록 정해진 것은 아니다. 다만 찾고자 하는 것에 1을 붙이는  것이 일반적이다.x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]y_data = [[0], [0], [0], [1], [1], [1]]# placeholders for a tensor that will be always fed.X = tf.placeholder(tf.float32, shape=[None, 2])Y = tf.placeholder(tf.float32, shape=[None, 1])W = tf.Variable(tf.random_normal([2, 1]), name='weight')b = tf.Variable(tf.random_normal([1]), name='bias')# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))# Linear Regression 의 Wx+b라는 공식을 사용하면 W가 1/2 였을 때, x의 값이 100인 경우  50이라는 높은 값이 나옴.# 0과 1만을 사용해야 하는데, 범위를 벗어나는 값이 나오게 된다.# 50보다 작으면 0, 크면 1이라고 표현하거나 1/2보다 작으면 0, 크면 1이라고 표현할 수  있는 추가 코드가 필요함.# 그래서 시그모이드를 사용하며 시그모이드는 Linear Regression 에서 가져온 값을 0과 1  사이의 값으로 변환한다.hypothesis = tf.sigmoid(tf.matmul(X, W) + b)# cost/loss function# 시그모이드 함수의 그래프를 보면 매끄럽지 않고 울퉁불퉁한 그래프가 나오기 때문에 그걸  펴주지 않으면 global minimum 이 아닌 local minimum 에서 최저점으로 인식해버릴 수  있기 때문에 새로운 코스트 함수가 필요함.# 그래프를 피기 위해서 로그가 등장한다.# 원래는 -log(H(x)) - (y == 1) / -log(1-H(x)) - (y == 0) 두 식이 필요한데  둘을 합친게 아래의 코스트 식이다.# y == 1그래프가 전체 그래프의 왼쪽을, y == 0이 오른쪽을 담당한다cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)# Accuracy computation# True if hypothesis&gt;0.5 else Falsepredicted = tf.cast(hypothesis &gt; 0.5, dtype=tf.float32)accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))# Launch graphwith tf.Session() as sess:   # Initialize TensorFlow variables   sess.run(tf.global_variables_initializer())   for step in range(10001):       cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})       if step % 200 == 0:           print(step, '\t', cost_val)   # Accuracy report   h, c, a = sess.run([hypothesis, predicted, accuracy],                      feed_dict={X: x_data, Y: y_data})   print(\"\nHypothesis: \n\", h, \"\nCorrect (Y): \n\", c, \"\nAccuracy: \n\", a)"
                         
                       } ,
                     
                       {
                         
                            "title"    : "Softmax classification",
                            "category" : "",
                            "tags"     : " TensorFlow",
                            "url"      : "/mckaydev.github.io/portfolio/3softmax",
                            "date"     : "March 15, 2013",
                            "excerpt"  : "",
                            "content"  : "&lt;Softmax&gt;import tensorflow as tfimport numpy as np# 로지스틱과는 달리 여러개의 label 를 갖는 multinomial classification 을 손쉽게 구현  할 수 있음# 1과 0이 아닌 a b c중 선택지라면 로지스틱은 여러개의 biniary classification 이  필요하지만 softmax 는 로지스틱의 행렬들을 하나의 행렬로 결합하여 사용하기 때문에  변수 1개로 처리가 가능하다xy = np.loadtxt('05train.txt', delimiter=',', dtype=np.float32)x_data = xy[:, 0:4]y_data = xy[:, 4:]# x의 데이터 갯수는 4개X = tf.placeholder(\"float\", [None, 4])# y의 데이터 갯수는 a b c 셋중 하나이기 때문에 값을 3개 준다. a에 해당하면 1 0 0 으로  표현한다.Y = tf.placeholder(\"float\", [None, 3])nb_classes = 3# softmax는 binary classification을 여러 번 결합한 결과다. 예측 결과가 A, B, C 중의  하나가 되어야 한다면, 동일한 x에 대해 A가 될 확률, B가 될 확률, C가 될 확률을 모두  구해야 한다. x는 3번 사용되지만, A, B, C에 대해서 한 번씩 필요하니까 3번 반복된다.  그래서, W는 예측해야 하는 숫자만큼 필요하게 된다.# 10개 중에서 예측한다면 10개의 W가 나와야 한다.# 여기선 x의 데이터 갯수가 4개 이기때문에 W도 4개를 예측하여야 한다.W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')b = tf.Variable(tf.random_normal([nb_classes]), name='bias')# tf.nn.softmax computes softmax activations# softmax = exp(logits) / reduce_sum(exp(logits), dim)# softmax 의 두가지 역할# 1. 입력을 sigmoid와 마찬가지로 0과 1사이의 값으로 변환한다.# 2. 변환된 결과에 대한 합계가 1이 되도록 만들어 준다.hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)# Cross entropy cost/loss# cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이  줄어드는 방향으로, 즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다.# cost-entropy cost 함수 저기의 Y * tf.log(hy-)는 L x log(S)를 나타낸 것으로써# 행렬간의 곱셈이 아닌 element-wise 곱셈이다.# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))# Y * tf.log(hypothesis) 결과는 행 단위로 더해야 한다.# 그림에서 보면, 최종 cost를 계산하기 전에 행 단위로 결과를 더하고 있다.# 이것을 가능하게 하는 옵션이 reduction_indices 매개변수다.# 0을 전달하면 열 합계, 1을 전달하면 행 합계, 아무 것도 전달하지 않으면 전체 합계.# 이렇게 행 단위로 더한 결과에 대해 전체 합계를 내서 평균을 구하기 위해 reduce_mean 함수가 사용됐다.# reduction_indices has been deprecated(더이상 사용되지 않는). Better to use axis instead of.# If the axis is not set, reduces all its dimensions.cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)# Launch graphwith tf.Session() as sess:    sess.run(tf.global_variables_initializer())    for step in range(2001):        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})        if step % 200 == 0:            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))    # 학습한 결과를 토대로 학점을 예측하고 있는데 결과에 따라 0 1 2를 반환한다.    # argmax 함수는 one-hot encoding 을 구현하는 텐서 함수.      arg_max는 더이상 사용되지 않음.    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9],                                              [1, 3, 4, 3],                                              [1, 1, 0, 1]]})    print(all, sess.run(tf.argmax(all, 1)))"
                         
                       } ,
                     
                       {
                         
                            "title"    : "Fancy Softmax classification",
                            "category" : "",
                            "tags"     : " TensorFlow",
                            "url"      : "/mckaydev.github.io/portfolio/4fancysoftmax",
                            "date"     : "March 16, 2013",
                            "excerpt"  : "",
                            "content"  : "&lt;Fancy Softmax classification&gt;import tensorflow as tfimport numpy as np# Predicting animal type based on various featuresxy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)# 모든 행과 마지막 행을 뺀 2차원 배열이 x의 값x_data = xy[:, 0:-1]# 모든 행과 마지막 행을 가진 배열이 y의 값y_data = xy[:, [-1]]# y의 종류 갯수 (동물의 개채종 숫자)nb_classes = 7  # 0 ~ 6# x의 속성값들. (다리의 갯수나 알을 낳는지 등등의 16개 속성값)X = tf.placeholder(tf.float32, [None, 16])# y의 속성값. (무슨 종이냐라는 속성값 하나밖에 존재하지 않음으로 1)Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6# one_hot 이 아니기에 최대값만 1로 설정해주고 나머지는 0으로 만드는 one_hot 으로 변환# Y(0에서 6까지의 수)를 넣고, 데이터들이 몇개의 클래스 인지(종의 숫자)를 알려줘야 함.Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot# one_hot 으로 바꿨을 때 배열에 한차원이 추가되기 때문에 차원을 다시 하나 줄여주는 기능# one_hot rank N을 변환시키면 rank 가 N + 1이 되어버림. 예를들어# [[0], [3]] 을 변환시키면 [[[1000000]], [[0001000]]] 이렇게 대괄호가 추가되어 아웃풋    됨.# reshape 에 one_hot 결과물을 집어 넣고 행은 '-1' 즉 알아서 하고 열은 7인 2차원 배열로    만들어 달라고 요구Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])# W는 x에 곱해지는 값이기 때문에 x의 열이 16이니 W의 행이 16, 거기에 무슨 종인지 대입해봐야  하기 때문에 종의 갯수 열)W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')# Y에 더해지는 값. W와 x의 곱이 [None, nb_classes]이기 때문에 그 결과의 열에 더해질 랜덤의  값들이 때문에 1차원 배열이며 크기는 nb_classes 인 배열로 선언하고 랜덤값 집어넣음.b = tf.Variable(tf.random_normal([nb_classes]), name='bias')# tf.nn.softmax computes softmax activations# softmax = exp(logits) / reduce_sum(exp(logits), dim)# 기본적인 로짓 선언 초기화logits = tf.matmul(X, W) + b# 시그모이드가 아닌 softmax 를 사용hypothesis = tf.nn.softmax(logits)# Cross entropy cost/loss# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)# 위에 있는 기존의 Softmax 식 처럼 복잡했던 식을 문자만으로 간단하게 표현하기 위한 식.# 코스트를 계산하기 위해 one_hot 을 적용한 Y 데이터와 로짓을 넣어줌.cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)# 위의 코스트들을 평균내고 최종적인 코스트를 완성함.cost = tf.reduce_mean(cost_i)# 옵티마이저에게 그래디센을 이용하여 cost 를 최소화 하라고 명령.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)# 우리가 예측한 값. 예측한 값(확률)을 argmax 를 이용하여 0에서 6 사이의 값으로 만들어 줌prediction = tf.argmax(hypothesis, 1)# 실제의 값과 비교하는 연산.correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))# 맞게 예측을 한 것들을 모아서 평균을 내 accuracy 에 넣어 줌.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# Launch graphwith tf.Session() as sess:    sess.run(tf.global_variables_initializer())    for step in range(2000):        # 옵티마이저에 x_y_data 를 넣어서 러닝을 시킴.        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})        if step % 100 == 0:            loss, acc = sess.run([cost, accuracy], feed_dict={X: x_data, Y: y_data})            print(\"Step: {:5}\tLoss: {:.3f}\tAcc: {:.2%}\".format(step, loss, acc))    # Let's see if we can predict    pred = sess.run(prediction, feed_dict={X: x_data})    # y_data: (N,1) = flatten =&gt; (N, ) matches pred.shape    # flatten 이란 y의 값이 [[1],[0]]일때 [1, 0] 처럼 바꾸어 주는 기능.    # zip 이란 pred, y_data.flatten() 가 리스트이기 때문에 각각의 엘리먼트를 p와 y에      넘겨주기 편하게 하기 위해서 씀.    for p, y in zip(pred, y_data.flatten()):        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))"
                         
                       } ,
                     
                       {
                         
                            "title"    : "MNIST",
                            "category" : "",
                            "tags"     : " TensorFlow",
                            "url"      : "/mckaydev.github.io/portfolio/5mnist",
                            "date"     : "March 25, 2018",
                            "excerpt"  : "",
                            "content"  : "&lt;MNIST&gt;import tensorflow as tfimport matplotlib.pyplot as pltimport randomfrom tensorflow.examples.tutorials.mnist import input_data# Check out https://www.tensorflow.org/get_started/mnist/beginners for# more information about the mnist dataset# input_data 는 mnist 튜토리얼 자료mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)# 0부터 9까지의 숫자들이 결과로 나와야 하기 때문에 경우의 수는 10nb_classes = 10# MNIST 데이터의 이미지는 28 * 28 이므로 1차원으로 보면 784개X = tf.placeholder(tf.float32, [None, 784])# 라벨의 갯수, 0에서 9까지의 숫자들을 서로 각각의 확률(Softmax)을 계산해야 하기 때문에  10을 줌.# Softmax 는 여러 경우중 확률이 제일 높은 하나의 선택지를 고르는 것 이기에 모든 경우의  수가 포함되어야 함Y = tf.placeholder(tf.float32, [None, nb_classes])# x에 대한 가중치 이므로 x의 열이 784이니 W의 행이 784W = tf.Variable(tf.random_normal([784, nb_classes]))# 결과값이 10개가 나오니 거기에 더해줄 bias 값도 10개가 있어야 함.b = tf.Variable(tf.random_normal([nb_classes]))# Hypothesis (using softmax) - 일반적인 Softmax 함수의 가설# 딥러닝에서는 일정기준을 만족시킬 때 활성화 되기 때문에 activation 함수라고 부른다.# 검은색의 농도를 측정하여 계산하는게 아닌 색깔의 유무라는 단순한 구조이기에 전과 같은  식을 사용.hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)# Test modelis_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))# Calculate accuracyaccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))# parameterstraining_epochs = 15# 한번에 100개씩의 이미지를 처리합니다.batch_size = 100with tf.Session() as sess:    # Initialize TensorFlow variables    sess.run(tf.global_variables_initializer())    # Training cycle    # 15번 반복하는 포문    for epoch in range(training_epochs):        avg_cost = 0        # num_examples 가 반환하는 값은 55,000        # 여기서 나누어 떨어지지 않으면 뒤쪽의 나머지 이미지들은 사용하지 않는다.        # mnist 는 train, test, validation 3개의 데이터 셋이 있음.        total_batch = int(mnist.train.num_examples / batch_size)        # (55,000/100=550)번 반복하는 포문        for i in range(total_batch):            # next_batch 함수는 지정한 수만큼 image 와 label 을 순차적으로 반환하는 함수.            # batch_xs.shape = (100, 784), batch_ys.shape = (100, 10)            # def next_batch(num, data, labels):            #     '''            #     Return a total of `num` random samples and labels.            #     '''            #     idx = np.arange(0, len(data))            #     np.random.shuffle(idx)            #     idx = idx[:num]            # Type 1            #     data_shuffle = [data[i] for i in idx]            #     labels_shuffle = [labels[i] for i in idx]            #            #     return np.asarray(data_shuffle), np.asarray(labels_shuffle)            # Type 2            #     data_shuffle = data[idx]            #     labels_shuffle = labels[idx]            #     labels_shuffle = np.asarray(labels_shuffle.values.reshape(len(labels_shuffle), 1))            #            #     return data_shuffle, labels_shuffle            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})            # 100개씩 나눠서 계산하기 때문에 cost 의 값을 계속 누적시켜준다.            # epoch 하나를 돌았을 때의 코스트를 계산하기 위하여 550으로 나눠줌.            avg_cost += c / total_batch        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))    # 학습을 다 했으니 mnist.test 데이터 셋을 가지고 테스트를 진행하는 코드.    # 테스트 데이터에서 랜덤으로 값 하나를 뽑아옴.    r = random.randint(0, mnist.test.num_examples - 1)    # 그리고 [r:r + 1]을 통해서(슬라이싱) r번째의 image 와 label 하나를 가져온다.    # argmax 는 가장 긑 값을 찾아서 1로 변화하는 one-hot encoding 알고리즘을 사용하게      해주는 함수.    # 이 함수를 거치면 10개의 데이터중 가장 큰 확률을 가지는 요소만 1이 되고 나머지는      0이 된다.    print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))    print(\"Prediction:\", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))    # 784개의 1차원 배열로 되어있는 이미지를 28*28로 reshape 해준다.    plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')    plt.show()"
                         
                       } 
                     
                    
                  ],
            searchResultTemplate: '<div class="search-title"><a href="{url}"><h3> {title}</h3></a><div class="meta">{date} <div class="right"><i class="fa fa-tag"></i> {tags}</div></div><p>{excerpt}</p></div><hr> ',
            noResultsText: 'No results found',
            limit: 10,
            fuzzy: false,
            exclude: []
        })
    </script>
</section>
</section>
    
    
  <!-- Tag list for portfolio -->
  
  


<footer>
  <div class="tag-list"></div>
</footer>

    
</article>

    </div>
    
<footer class="site-footer">
    <p class="text">Powered by <a href="https://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/sylhare/Type-on-Strap">Type on Strap</a>
</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                
<li>
	<a href="http://localhost:4000/mckaydev.github.io/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>



<li>
	<a href="mailto:simi7236@gmail.com" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>













<li>
	<a href="https://github.com/mckaydev" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>





<li>
	<a href="http://instagram.com/mckay_lee/" title="Follow on Instagram">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-instagram fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>















<li>
	<a href="http://steamcommunity.com/id/simi7236" title="Follow on Steam">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-steam fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>














                </ul>
            </div>
</footer>




  </body>
</html>
